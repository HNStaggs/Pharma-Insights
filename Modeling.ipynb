{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26d1edf",
   "metadata": {},
   "source": [
    "# Load Libraries and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b84b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System, Data, Time, and Spec Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np \n",
    "from line_profiler import LineProfiler  # Code peformance\n",
    "profiler = LineProfiler()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import csv\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "import multiprocess as mp\n",
    "num_cores = mp.cpu_count()\n",
    "\n",
    "# Data Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "#from pandas.io.json import json_normalize  # Older version\n",
    "from pandas import json_normalize  # Newer version\n",
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "\n",
    "# Natural Language Processing Libraries\n",
    "import json\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import words\n",
    "import string\n",
    "import nltk\n",
    "from collections import OrderedDict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pandas import json_normalize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "punctuation = set(punctuation)\n",
    "punctuation.update({'_', '-','â€˜'})\n",
    "english_words = set(words.words())\n",
    "from fuzzywuzzy import process\n",
    "#nltk.download('words')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# SQL Interface Libraries\n",
    "import pymysql as mysql\n",
    "import mysql.connector\n",
    "import pyodbc\n",
    "import sqlite3\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import event\n",
    "from string import punctuation\n",
    "\n",
    "# Machine Learning Libraries\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import yeojohnson\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import OneHotEncoder, PowerTransformer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay,roc_auc_score, roc_curve \n",
    "from sklearn.metrics import classification_report, mean_squared_error, f1_score\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from dmba import classificationSummary, AIC_score, BIC_score, plotDecisionTree,gainsChart\n",
    "from scikitplot.metrics import plot_lift_curve, plot_cumulative_gain\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from imblearn.metrics import specificity_score, sensitivity_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from scikitplot.metrics import plot_lift_curve\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from patsy import dmatrices\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import KFold\n",
    "from dmba import stepwise_selection, classificationSummary, backward_elimination\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Importing Custom Functions\n",
    "import nbimporter\n",
    "from Functions import nan_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6f85a2",
   "metadata": {},
   "source": [
    "# Get Dataset from SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd9e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the MySQL server\n",
    "connection = mysql.connector.connect(\n",
    "    host=\"localhost\", user=\"root\", password=PASSWORD, database=\"pharma_db\"\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c46ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master Query from Data\n",
    "\n",
    "master_query = \"\"\"SELECT \n",
    "                a.serious_outcome,\n",
    "                a.expedited,\n",
    "                a.age,\n",
    "                a.sex,\n",
    "                a.year\n",
    "                a.weight\n",
    "                r.outcome,\n",
    "                p.unit_price,\n",
    "                p.generic_brand,\n",
    "                l.ingredients,\n",
    "                l.rxcui,\n",
    "                l.set_id,\n",
    "                d.manu_num,\n",
    "                d.unii\n",
    "            FROM adverse_events a \n",
    "                INNER JOIN patients_reactions r ON a.event_id = r.event_id \n",
    "                INNER JOIN patients_drugs d ON r.event_id = d.event_id \n",
    "                INNER JOIN prices p ON d.ndc11 = p.ndc11\n",
    "                INNER JOIN lables l ON p.ndc11 = l.ndc11\n",
    "            WHERE y.year, a.year, etc. FROM 2020 TO 2024  # Create proper syntax here\n",
    "            ORDER BY y.year\"\"\"  # Still need to test and figure out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1721f5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(master_query)\n",
    "result = cursor.fetchall()\n",
    "column_names = [i[0] for i in cursor.description]\n",
    "master_query_df = pd.DataFrame(result, columns=column_names)\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feff3bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d0a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e18d54f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fc933d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80cb749f",
   "metadata": {},
   "source": [
    "# Preparation for Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff08906",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_query_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92ebb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_info(master_query_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb1a292",
   "metadata": {},
   "source": [
    "### Update Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15b7c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_query_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba278f65",
   "metadata": {},
   "source": [
    "### Define numerical, text, and categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d2e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['drug_name']\n",
    "nums = ['age', 'weight', 'price'] \n",
    "texts = ['']\n",
    "all_vars = cats+nums+texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fd9a25",
   "metadata": {},
   "source": [
    "### Create Transformation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e0fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a categorical processing pipeline that uses one-hot encoding\n",
    "# Dropping binary columns and drop first of each level** NEED TO ADD**\n",
    "cat_pipe = Pipeline([('encoder', OneHotEncoder(drop='if_binary'))])\n",
    "\n",
    "# Create a numerical processing pipeline that uses skewness correction/center/scale.\n",
    "num_pipe = Pipeline([('skew_standardize', PowerTransformer())])\n",
    "\n",
    "# Create a text token processing step to vectorize tokens\n",
    "text_pipe = Pipeline([('vector', tf_idf function())])\n",
    "\n",
    "# Combine pipeline steps\n",
    "all_pipe = make_pipeline(ColumnTransformer([('cat', cat_pipe, cats), \n",
    "                                            ('num', num_pipe, nums),\n",
    "                                           ('text', text_pipe, texts)],\n",
    "                                          verbose_feature_names_out=False))\n",
    "# Verify steps\n",
    "all_pipe.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b208e",
   "metadata": {},
   "source": [
    "# Split Data into Training/Validation/Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f9eef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and the target variable (y).\n",
    "X = master_query_df[[all_vars]]\n",
    "\n",
    "#Define outcome variable\n",
    "y = master_query_df[['outcome']]  # Need to Decide 5 Level or 3 Level\n",
    "\n",
    "#Split data\n",
    "X_train, X_rest, y_train, y_rest = train_test_split(X, y, train_size=0.8, random_state = 2)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_rest, y_rest, test_size=0.5, random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852d4e27",
   "metadata": {},
   "source": [
    "## Apply Pipeline to Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92115f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit pipeline to resampled data\n",
    "X_train_fit = all_pipe.fit(X_train)\n",
    "#Get feature names out from fit and create as new list\n",
    "X_train_cols = X_train_fit.get_feature_names_out().tolist()\n",
    "X_train_pipe = pd.DataFrame(all_pipe.fit_transform(X_train), columns = X_train_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16f586b",
   "metadata": {},
   "source": [
    "## Apply Pipeline to Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa46837",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit pipeline to resampled data\n",
    "X_val_fit = all_pipe.fit(X_val)\n",
    "#Get feature names out from fit and create as new list\n",
    "X_val_cols = X_val_fit.get_feature_names_out().tolist()\n",
    "X_val_pipe = pd.DataFrame(all_pipe.fit_transform(X_val), columns = X_val_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4b7659",
   "metadata": {},
   "source": [
    "## Apply Pipeline to Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025d22fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit pipeline to resampled data\n",
    "X_test_fit = all_pipe.fit(X_test)\n",
    "#Get feature names out from fit and create as new list\n",
    "X_test_cols = X_test_fit.get_feature_names_out().tolist()\n",
    "X_test_pipe = pd.DataFrame(all_pipe.fit_transform(X_test), columns = X_test_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e207ea03",
   "metadata": {},
   "source": [
    "## Undersample Training Data to Balance Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5fdd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RandomUnderSampler instance with a specified random seed and sampling strategy\n",
    "rus = RandomUnderSampler(random_state = 1, sampling_strategy='majority')\n",
    "\n",
    "# Perform random under-sampling on the training dataset\n",
    "X_train_under, y_train_under = rus.fit_resample(X_train_pipe, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2ba158",
   "metadata": {},
   "source": [
    "# Multiclass Classification Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ed355f",
   "metadata": {},
   "source": [
    "## White Box Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b1a1d5",
   "metadata": {},
   "source": [
    "### Logistic Regression L1 w/ 10-k CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207a804a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Logistic Regression model with L2 regularization\n",
    "log_l1 = LogisticRegressionCV(solver = 'saga', penalty = 'l1', cv = 10, random_state = 1)\n",
    "# Fit the model to the training data\n",
    "log_l1.fit(X_train_under, y_train_under.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b9838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intercept Log-Odds and Odds\n",
    "print(log_l1.intercept_ , np.exp(log_l1.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf7b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table of coefficient odds\n",
    "d = {'Feature': pd.Series(X_train_under.columns), 'LogOdds': pd.Series(log_l1.coef_[0])}\n",
    "df = pd.DataFrame(data=d).reindex(d['LogOdds'].abs().sort_values(ascending=False).index)\n",
    "df['Odds'] = np.exp(df['LogOdds'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed9e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross val performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5e3159",
   "metadata": {},
   "source": [
    "### Single Decision Tree with Grid Search and 10k-CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adfed8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a grid of hyperparameters to search for in tree\n",
    "param_grid = {\n",
    "    'max_depth' : [1,2,3,4,5],\n",
    "    'min_samples_leaf' : [1,2,3,4,5]\n",
    "    \n",
    "}\n",
    "# Create a GridSearchCV object using and the defined parameter grid\n",
    "tree1_search = GridSearchCV(DecisionTreeClassifier(random_state=1), param_grid, cv=10, n_jobs=-1)\n",
    "# Fit the GridSearchCV to the balanced training data to find the best hyperparameters\n",
    "tree1_search.fit(X_train_under, y_train_under.values.ravel())\n",
    "# Get the best hyperparameters found by the GridSearch\n",
    "tree1_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92029bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1 = DecisionTreeClassifier(max_depth = 4, min_samples_leaf = 4, random_state = 1).fit(X_train_under, y_train_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b21e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance scores\n",
    "importances=tree1.feature_importances_\n",
    "feature_importance_pairs=list(zip(X_train_under.columns, importances))\n",
    "\n",
    "# Sort in descending order\n",
    "sorted_feature_importance_pairs = sorted(feature_importance_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print scores\n",
    "print(\"Feature Importance Scores\")\n",
    "for feature, importance in sorted_feature_importance_pairs:\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb82979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross val performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19400b7",
   "metadata": {},
   "source": [
    "## Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467fa457",
   "metadata": {},
   "source": [
    "### Random Forest Classifier w/ 10-k CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023ff245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "# Create a Random Forest classifier with 100 trees\n",
    "random_forest = RandomForestClassifier(n_estimators=1000, random_state=1)  \n",
    "\n",
    "# Fit (train) the Random Forest classifier on the balanced training data\n",
    "random_forest.fit(X_train_under, y_train_under.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbecf79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance scores\n",
    "importances=random_forest.feature_importances_\n",
    "feature_importance_pairs=list(zip(X_train_under.columns, importances))\n",
    "\n",
    "# Sort in descending order\n",
    "sorted_feature_importance_pairs = sorted(feature_importance_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print scores\n",
    "print(\"Feature Importance Scores\")\n",
    "for feature, importance in sorted_feature_importance_pairs:\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6568058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature names and importances\n",
    "feature_names, importances = zip(*sorted_feature_importance_pairs)\n",
    "\n",
    "# Create a horizontal bar plot\n",
    "plt.figure(figsize=(14, 12))\n",
    "#plt.barh(range(len(feature_names)), importances, align='center')\n",
    "#plt.yticks(range(len(feature_names)), feature_names)\n",
    "\n",
    "plt.barh(range(len(feature_names)), importances, align='center')\n",
    "plt.yticks(range(len(feature_names)), feature_names)\n",
    "\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.gca().invert_yaxis()  # Invert the y-axis to show the most important features at the top\n",
    "#plt.show()\n",
    "plt.savefig('featimp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06b0e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross val peformance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5b1c42",
   "metadata": {},
   "source": [
    "## Gradient Boosted Tree w/ 10-k CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77389cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a grid of hyperparameters to search for in the Gradient Boosting Classifier\n",
    "param_grid = {\n",
    "    'max_depth' : [1,2,3,4,5],\n",
    "    'learning_rate' : [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'n_estimators' : [20,21,22,23,24],\n",
    "}\n",
    "# Create a GridSearchCV object using GradientBoostingClassifier and the defined parameter grid\n",
    "tree_search = GridSearchCV(GradientBoostingClassifier(random_state=1), param_grid, cv=10, n_jobs=-1)\n",
    "# Fit the GridSearchCV to the balanced training data to find the best hyperparameters\n",
    "tree_search.fit(X_train_under, y_train_under.values.ravel())\n",
    "# Get the best hyperparameters found by the GridSearch\n",
    "tree_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f98b9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross val performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b13cc0",
   "metadata": {},
   "source": [
    "## Non-parametric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c8fe6c",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41251dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store KNN results\n",
    "knn_results = []\n",
    "for k in range (1, 50):\n",
    "    # Create a K-Nearest Neighbors model with k neighbors and fit it to the balanced training data\n",
    "    knn_mod = KNeighborsClassifier(n_neighbors = k).fit(X_train_under, y_train_under.values.ravel())\n",
    "    # Calculate and append results to the list\n",
    "    knn_results.append({\n",
    "        'k': k,\n",
    "        'Sens': sensitivity_score(y_test_imp.values.ravel(), knn_mod.predict(X_test_combo)),\n",
    "        'Acc': accuracy_score(y_test_imp.values.ravel(), knn_mod.predict(X_test_combo)) \n",
    "    })\n",
    "# Create a DataFrame from the list of KNN results    \n",
    "knn_results = pd.DataFrame(knn_results)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fa3df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27848ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a K-Nearest Neighbors (KNN) classifier with 6 neighbors and fit it to the balanced training data\n",
    "knn = KNeighborsClassifier(n_neighbors = 5).fit(X_train_under, y_train_under.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f3a01c",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f67e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0743265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c723211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4aceaf",
   "metadata": {},
   "source": [
    "# Validation Data Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322cba06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c9f288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed9d83de",
   "metadata": {},
   "source": [
    "# Testing Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383afe78",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465bfbcc",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a632a6e",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc726260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "log_cm = confusion_matrix(y_true = y_test, y_pred = log_l1.predict(X_test_log), labels = log_l2_bal.classes_)\n",
    "log_disp = ConfusionMatrixDisplay(confusion_matrix= log_cm_bal, display_labels=log_l2_bal.classes_)\n",
    "log_disp.plot()\n",
    "#plt.show()\n",
    "plt.savefig('lr_cm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4678622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ced673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585610cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sensitivity, specificity, and accuracy for log reg\n",
    "logl1_sens = sensitivity_score(y_test, log_l1.predict(X_test))\n",
    "logl1_spec = specificity_score(y_test, log_l1.predict(X_test))\n",
    "logl1_acc = accuracy_score(y_test, log_l1.predict(X_test))\n",
    "logl1_prec = precision_score(y_test, log_l1.predict(X_test)) \n",
    "logl1_rec = recall_score(y_test, log_l1.predict(X_test))\n",
    "logl1_f1 = f1_score(y_test, log_l1.predict(X_test))\n",
    "logl1_sens, logl1_spec, logl1_acc, logl1_prec, logl1_rec, logl1_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6215bd",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0823003b",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0c877",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd251fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix for tree\n",
    "tree_cm = confusion_matrix(y_true = y_test, y_pred = tree1.predict(X_test), labels = tree1.classes_)\n",
    "tree_disp = ConfusionMatrixDisplay(confusion_matrix= tree_cm, display_labels=tree1.classes_)\n",
    "tree_disp.plot()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfbc056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311da40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sensitivity, specificity, and accuracy for decision tree\n",
    "tree1_sens = sensitivity_score(y_test, tree1.predict(X_test))\n",
    "tree1_spec = specificity_score(y_test, tree1.predict(X_test))\n",
    "tree1_acc = accuracy_score(y_test, tree1.predict(X_test))\n",
    "tree1_prec = precision_score(y_test, tree1.predict(X_test)) \n",
    "tree1_rec = recall_score(y_test, tree1.predict(X_test))\n",
    "tree1_f1 = f1_score(y_test, tree1.predict(X_test))\n",
    "tree1_sens, tree1_spec, tree1_acc, tree1_prec, tree1_rec, tree1_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6951a4",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d161362",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c35829",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb01e6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix for RF \n",
    "rf_cm = confusion_matrix(y_true = y_test, y_pred = random_forest.predict(X_test), labels = random_forest.classes_)\n",
    "# Create a ConfusionMatrixDisplay object for visualization\n",
    "rf_disp = ConfusionMatrixDisplay(confusion_matrix= rf_cm, display_labels=random_forest.classes_)\n",
    "rf_disp.plot()\n",
    "warnings.filterwarnings('ignore')\n",
    "#plt.show()\n",
    "#plt.savefig('rf_cm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b073618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d77c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49636336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sensitivity, specificity, and accuracy for RF\n",
    "rf_sens = sensitivity_score(y_test, random_forest.predict(X_test))\n",
    "rf_spec = specificity_score(y_test, random_forest.predict(X_test))\n",
    "rf_acc = accuracy_score(y_test, random_forest.predict(X_test))\n",
    "rf_prec = precision_score(y_test, random_forest.predict(X_test)) \n",
    "rf_rec = recall_score(y_test, random_forest.predict(X_test))\n",
    "rf_f1 = f1_score(y_test, random_forest.predict(X_test))\n",
    "warnings.filterwarnings('ignore')\n",
    "rf_sens, rf_spec, rf_acc, rf_prec, rf_rec, rf_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2109a84",
   "metadata": {},
   "source": [
    "## Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6352e750",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93411af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d5bc0b8",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ae547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix for tree\n",
    "tree_cm = confusion_matrix(y_true = y_test, y_pred = tree_search.predict(X_test), labels = tree_search.classes_)\n",
    "tree_disp = ConfusionMatrixDisplay(confusion_matrix= tree_cm, display_labels=tree_search.classes_)\n",
    "tree_disp.plot()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddb565c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8c6498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc0668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sensitivity, specificity, and accuracy for decision tree \n",
    "tree_sens = sensitivity_score(y_test, tree_search.predict(X_test))\n",
    "tree_spec = specificity_score(y_test, tree_search.predict(X_test))\n",
    "tree_acc = accuracy_score(y_test, tree_search.predict(X_test))\n",
    "tree_prec = precision_score(y_test, tree_search.predict(X_test)) \n",
    "tree_rec = recall_score(y_test, tree_search.predict(X_test))\n",
    "tree_f1 = f1_score(y_test, tree_search.predict(X_test))\n",
    "tree_sens, tree_spec, tree_acc, tree_prec, tree_rec, tree_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392f0ac",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a16ecfd",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d10f2d",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a57ae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "# Compute the confusion matrix for KNN \n",
    "knn_cm = confusion_matrix(y_true = y_test, y_pred = knn.predict(X_test), labels = knn.classes_)\n",
    "# Create a ConfusionMatrixDisplay object for visualization\n",
    "knn_disp = ConfusionMatrixDisplay(confusion_matrix= knn_cm, display_labels=knn.classes_)\n",
    "knn_disp.plot()\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da866fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e376e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4f3552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sensitivity, specificity, and accuracy for KNN \n",
    "knn_sens = sensitivity_score(y_test, knn.predict(X_test))\n",
    "knn_spec = specificity_score(y_test, knn.predict(X_test))\n",
    "knn_acc = accuracy_score(y_test, knn.predict(X_test))\n",
    "knn_prec = precision_score(y_test, knn.predict(X_test)) \n",
    "knn_rec = recall_score(y_test, knn.predict(X_test))\n",
    "knn_f1 = f1_score(y_test, knn.predict(X_test))\n",
    "warnings.filterwarnings('ignore')\n",
    "knn_sens, knn_spec, knn_acc, knn_prec, knn_rec, knn_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d02998",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ca31d3",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a9dadc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "394a9db7",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e910df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf66096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6290d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48969e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a35ac01",
   "metadata": {},
   "source": [
    "# Performance Metrics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b275bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table to display performance metrics for different models on the validation dataset\n",
    "\n",
    "val_performance = [\n",
    "{'Model': 'Neural Net', 'Test Sensitivity': nn_sens, 'Test Specificity':nn_spec, \n",
    " 'Accuracy': nn_acc, 'Test Precision': nn_prec, 'Test Recall': nn_rec, 'F1 Score': nn_f1},\n",
    "    {'Model': 'Logistic Regression', 'Test Sensitivity': logl1_sens, 'Test Specificity':logl1_spec, \n",
    " 'Accuracy': logl1_acc, 'Test Precision': logl1_prec, 'Test Recall': logl1_rec,  'F1 Score': logl1_f1},\n",
    "    {'Model': 'Boosted Tree', 'Test Sensitivity': tree_sens, 'Test Specificity':tree_spec, \n",
    " 'Accuracy': tree_acc, 'Test Precision': tree_prec, 'Test Recall': tree_rec,  'F1 Score': tree_f1},\n",
    "    {'Model': 'Single Tree', 'Test Sensitivity': tree1_sens, 'Test Specificity':tree1_spec, \n",
    " 'Accuracy': tree1_acc, 'Test Precision': tree1_prec, 'Test Recall': tree1_rec, 'F1 Score': tree1_f1},\n",
    "    {'Model': 'Random Forest', 'Test Sensitivity': rf_sens, 'Test Specificity':rf_spec, \n",
    " 'Accuracy': rf_acc, 'Test Precision': rf_prec, 'Test Recall': rf_rec, 'F1 Score': rf_f1},\n",
    "    {'Model': 'K-Nearest Neighbors', 'Test Sensitivity': knn_sens, 'Test Specificity':knn_spec, \n",
    " 'Accuracy': knn_acc, 'Test Precision': knn_prec, 'Test Recall': knn_rec,  'F1 Score': knn_f1},\n",
    "    \n",
    "]\n",
    "# Create a formatted table using tabulate and specify the format as 'fancy_grid'\n",
    "table = tabulate(val_performance, headers='keys', tablefmt='fancy_grid')\n",
    "# Display the comparison table\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd3d311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdfee41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302409a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
