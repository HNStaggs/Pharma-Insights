{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6688cc1c",
   "metadata": {},
   "source": [
    "# Load Libraries and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bce998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System, Data, Time, and Spec Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np \n",
    "from line_profiler import LineProfiler  # Code peformance\n",
    "profiler = LineProfiler()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import csv\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "import multiprocess as mp\n",
    "num_cores = mp.cpu_count()\n",
    "\n",
    "# Data Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "#from pandas.io.json import json_normalize  # Older version\n",
    "from pandas import json_normalize  # Newer version\n",
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "\n",
    "# Natural Language Processing Libraries\n",
    "import json\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import words\n",
    "import string\n",
    "import nltk\n",
    "from collections import OrderedDict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pandas import json_normalize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "punctuation = set(punctuation)\n",
    "punctuation.update({'_', '-','â€˜'})\n",
    "english_words = set(words.words())\n",
    "from fuzzywuzzy import process\n",
    "#nltk.download('words')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# SQL Interface Libraries\n",
    "import pymysql as mysql\n",
    "import mysql.connector\n",
    "import pyodbc\n",
    "import sqlite3\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import event\n",
    "from string import punctuation\n",
    "\n",
    "# Machine Learning Libraries\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import yeojohnson\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import OneHotEncoder, PowerTransformer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay,roc_auc_score, roc_curve \n",
    "from sklearn.metrics import classification_report, mean_squared_error, f1_score\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from dmba import classificationSummary, AIC_score, BIC_score, plotDecisionTree,gainsChart\n",
    "from scikitplot.metrics import plot_lift_curve, plot_cumulative_gain\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from imblearn.metrics import specificity_score, sensitivity_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from scikitplot.metrics import plot_lift_curve\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from patsy import dmatrices\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import KFold\n",
    "from dmba import stepwise_selection, classificationSummary, backward_elimination\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Importing Custom Functions\n",
    "import nbimporter\n",
    "from Functions import nan_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eef2d27",
   "metadata": {},
   "source": [
    "# Get Dataset from SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efed114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the MySQL server\n",
    "connection = mysql.connector.connect(\n",
    "    host=\"localhost\", user=\"root\", password=PASSWORD, database=\"pharma_db\"\n",
    ")\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e3f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master Query from Data\n",
    "\n",
    "master_query = \"\"\"SELECT \n",
    "                a.serious_outcome,\n",
    "                a.expedited,\n",
    "                a.age,\n",
    "                a.sex,\n",
    "                a.year\n",
    "                a.weight\n",
    "                r.outcome,\n",
    "                p.unit_price,\n",
    "                p.generic_brand,\n",
    "                l.ingredients,\n",
    "                l.rxcui,\n",
    "                l.set_id,\n",
    "                d.manu_num,\n",
    "                d.unii\n",
    "            FROM adverse_events a \n",
    "                INNER JOIN patients_reactions r ON a.event_id = r.event_id \n",
    "                INNER JOIN patients_drugs d ON a.event_id = d.event_id \n",
    "                INNER JOIN prices p ON d.ndc11 = p.ndc11\n",
    "                INNER JOIN lables l ON p.ndc11 = l.ndc11 \n",
    "            ORDER BY y.year\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0eed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(master_query)\n",
    "result = cursor.fetchall()\n",
    "column_names = [i[0] for i in cursor.description]\n",
    "master_query_df = pd.DataFrame(result, columns=column_names)\n",
    "cursor.close()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a441b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c3fdc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2a655c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f77df96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05d134e6",
   "metadata": {},
   "source": [
    "# Preparation for Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad9a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_query_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9921f3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_info(master_query_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031583d7",
   "metadata": {},
   "source": [
    "### Update Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443d6087",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_query_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d42410",
   "metadata": {},
   "source": [
    "### Define numerical, text, and categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e893fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['drug_name']\n",
    "nums = ['age', 'weight', 'price'] \n",
    "texts = ['']\n",
    "all_vars = cats+nums+texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b3bc6",
   "metadata": {},
   "source": [
    "### Create Transformation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa65abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a categorical processing pipeline that uses one-hot encoding\n",
    "# Dropping binary columns and drop first of each level** NEED TO ADD**\n",
    "cat_pipe = Pipeline([('encoder', OneHotEncoder(drop='if_binary'))])\n",
    "\n",
    "# Create a numerical processing pipeline that uses skewness correction/center/scale.\n",
    "num_pipe = Pipeline([('skew_standardize', PowerTransformer())])\n",
    "\n",
    "# Create a text token processing step to vectorize tokens\n",
    "text_pipe = Pipeline([('vector', tf_idf function())])\n",
    "\n",
    "# Combine pipeline steps\n",
    "all_pipe = make_pipeline(ColumnTransformer([('cat', cat_pipe, cats), \n",
    "                                            ('num', num_pipe, nums),\n",
    "                                           ('text', text_pipe, texts)],\n",
    "                                          verbose_feature_names_out=False))\n",
    "# Verify steps\n",
    "all_pipe.named_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f643c28",
   "metadata": {},
   "source": [
    "# Split Data into Training/Validation/Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b5817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and the target variable (y).\n",
    "X = master_query_df[[all_vars]]\n",
    "\n",
    "#Define outcome variable\n",
    "y = master_query_df[['outcome']]  # Need to Decide 5 Level or 3 Level\n",
    "\n",
    "#Split data\n",
    "X_train, X_rest, y_train, y_rest = train_test_split(X, y, train_size=0.8, random_state = 2)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_rest, y_rest, test_size=0.5, random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17818e7f",
   "metadata": {},
   "source": [
    "## Apply Pipeline to Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb93021",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit pipeline to resampled data\n",
    "X_train_fit = all_pipe.fit(X_train)\n",
    "#Get feature names out from fit and create as new list\n",
    "X_train_cols = X_train_fit.get_feature_names_out().tolist()\n",
    "X_train_pipe = pd.DataFrame(all_pipe.fit_transform(X_train), columns = X_train_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408bf944",
   "metadata": {},
   "source": [
    "## Apply Pipeline to Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1e17d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit pipeline to resampled data\n",
    "X_val_fit = all_pipe.fit(X_val)\n",
    "#Get feature names out from fit and create as new list\n",
    "X_val_cols = X_val_fit.get_feature_names_out().tolist()\n",
    "X_val_pipe = pd.DataFrame(all_pipe.fit_transform(X_val), columns = X_val_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49810a41",
   "metadata": {},
   "source": [
    "## Apply Pipeline to Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895267fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit pipeline to resampled data\n",
    "X_test_fit = all_pipe.fit(X_test)\n",
    "#Get feature names out from fit and create as new list\n",
    "X_test_cols = X_test_fit.get_feature_names_out().tolist()\n",
    "X_test_pipe = pd.DataFrame(all_pipe.fit_transform(X_test), columns = X_test_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbf38e3",
   "metadata": {},
   "source": [
    "## Undersample Training Data to Balance Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaebfc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RandomUnderSampler instance with a specified random seed and sampling strategy\n",
    "rus = RandomUnderSampler(random_state = 1, sampling_strategy='majority')\n",
    "\n",
    "# Perform random under-sampling on the training dataset\n",
    "X_train_under, y_train_under = rus.fit_resample(X_train_pipe, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad7ab50",
   "metadata": {},
   "source": [
    "# Multiclass Classification Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65ff0ca",
   "metadata": {},
   "source": [
    "## White Box Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dab4015",
   "metadata": {},
   "source": [
    "### Logistic Regression L1 w/ 10-k CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1438970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Logistic Regression model with L2 regularization\n",
    "log_l1 = LogisticRegressionCV(solver = 'saga', penalty = 'l1', cv = 10, random_state = 1)\n",
    "# Fit the model to the training data\n",
    "log_l1.fit(X_train_under, y_train_under.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768aa387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intercept Log-Odds and Odds\n",
    "print(log_l1.intercept_ , np.exp(log_l1.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ca23e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table of coefficient odds\n",
    "d = {'Feature': pd.Series(X_train_under.columns), 'LogOdds': pd.Series(log_l1.coef_[0])}\n",
    "df = pd.DataFrame(data=d).reindex(d['LogOdds'].abs().sort_values(ascending=False).index)\n",
    "df['Odds'] = np.exp(df['LogOdds'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2399d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross val performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7946209",
   "metadata": {},
   "source": [
    "### Single Decision Tree with Grid Search and 10k-CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7264af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a grid of hyperparameters to search for in tree\n",
    "param_grid = {\n",
    "    'max_depth' : [1,2,3,4,5],\n",
    "    'min_samples_leaf' : [1,2,3,4,5]\n",
    "    \n",
    "}\n",
    "# Create a GridSearchCV object using and the defined parameter grid\n",
    "tree1_search = GridSearchCV(DecisionTreeClassifier(random_state=1), param_grid, cv=10, n_jobs=-1)\n",
    "# Fit the GridSearchCV to the balanced training data to find the best hyperparameters\n",
    "tree1_search.fit(X_train_under, y_train_under.values.ravel())\n",
    "# Get the best hyperparameters found by the GridSearch\n",
    "tree1_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f6f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1 = DecisionTreeClassifier(max_depth = 4, min_samples_leaf = 4, random_state = 1).fit(X_train_under, y_train_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e42574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance scores\n",
    "importances=tree1.feature_importances_\n",
    "feature_importance_pairs=list(zip(X_train_under.columns, importances))\n",
    "\n",
    "# Sort in descending order\n",
    "sorted_feature_importance_pairs = sorted(feature_importance_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print scores\n",
    "print(\"Feature Importance Scores\")\n",
    "for feature, importance in sorted_feature_importance_pairs:\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb067354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross val performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3787073",
   "metadata": {},
   "source": [
    "## Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2142ed27",
   "metadata": {},
   "source": [
    "### Random Forest Classifier w/ 10-k CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5428053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "# Create a Random Forest classifier with 100 trees\n",
    "random_forest = RandomForestClassifier(n_estimators=1000, random_state=1)  \n",
    "\n",
    "# Fit (train) the Random Forest classifier on the balanced training data\n",
    "random_forest.fit(X_train_under, y_train_under.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c1c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance scores\n",
    "importances=random_forest.feature_importances_\n",
    "feature_importance_pairs=list(zip(X_train_under.columns, importances))\n",
    "\n",
    "# Sort in descending order\n",
    "sorted_feature_importance_pairs = sorted(feature_importance_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print scores\n",
    "print(\"Feature Importance Scores\")\n",
    "for feature, importance in sorted_feature_importance_pairs:\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fc0451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature names and importances\n",
    "feature_names, importances = zip(*sorted_feature_importance_pairs)\n",
    "\n",
    "# Create a horizontal bar plot\n",
    "plt.figure(figsize=(14, 12))\n",
    "#plt.barh(range(len(feature_names)), importances, align='center')\n",
    "#plt.yticks(range(len(feature_names)), feature_names)\n",
    "\n",
    "plt.barh(range(len(feature_names)), importances, align='center')\n",
    "plt.yticks(range(len(feature_names)), feature_names)\n",
    "\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.gca().invert_yaxis()  # Invert the y-axis to show the most important features at the top\n",
    "#plt.show()\n",
    "plt.savefig('featimp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf401917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross val peformance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382dc13f",
   "metadata": {},
   "source": [
    "## Gradient Boosted Tree w/ 10-k CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4dee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a grid of hyperparameters to search for in the Gradient Boosting Classifier\n",
    "param_grid = {\n",
    "    'max_depth' : [1,2,3,4,5],\n",
    "    'learning_rate' : [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'n_estimators' : [20,21,22,23,24],\n",
    "}\n",
    "# Create a GridSearchCV object using GradientBoostingClassifier and the defined parameter grid\n",
    "tree_search = GridSearchCV(GradientBoostingClassifier(random_state=1), param_grid, cv=10, n_jobs=-1)\n",
    "# Fit the GridSearchCV to the balanced training data to find the best hyperparameters\n",
    "tree_search.fit(X_train_under, y_train_under.values.ravel())\n",
    "# Get the best hyperparameters found by the GridSearch\n",
    "tree_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded56cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross val performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb25cb58",
   "metadata": {},
   "source": [
    "## Non-parametric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a924b502",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e0925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store KNN results\n",
    "knn_results = []\n",
    "for k in range (1, 50):\n",
    "    # Create a K-Nearest Neighbors model with k neighbors and fit it to the balanced training data\n",
    "    knn_mod = KNeighborsClassifier(n_neighbors = k).fit(X_train_under, y_train_under.values.ravel())\n",
    "    # Calculate and append results to the list\n",
    "    knn_results.append({\n",
    "        'k': k,\n",
    "        'Sens': sensitivity_score(y_test_imp.values.ravel(), knn_mod.predict(X_test_combo)),\n",
    "        'Acc': accuracy_score(y_test_imp.values.ravel(), knn_mod.predict(X_test_combo)) \n",
    "    })\n",
    "# Create a DataFrame from the list of KNN results    \n",
    "knn_results = pd.DataFrame(knn_results)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba677935",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed84d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a K-Nearest Neighbors (KNN) classifier with 6 neighbors and fit it to the balanced training data\n",
    "knn = KNeighborsClassifier(n_neighbors = 5).fit(X_train_under, y_train_under.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a285e1b5",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0e97ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78baef6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b81216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03129070",
   "metadata": {},
   "source": [
    "# Validation Data Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e7100f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be403e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "206f26af",
   "metadata": {},
   "source": [
    "# Testing Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02225d4e",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7648d2b",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa4eb4a",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5ec47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "log_cm = confusion_matrix(y_true = y_test, y_pred = log_l1.predict(X_test_log), labels = log_l2_bal.classes_)\n",
    "log_disp = ConfusionMatrixDisplay(confusion_matrix= log_cm_bal, display_labels=log_l2_bal.classes_)\n",
    "log_disp.plot()\n",
    "#plt.show()\n",
    "plt.savefig('lr_cm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f57093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c2c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d82744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sensitivity, specificity, and accuracy for log reg\n",
    "logl1_sens = sensitivity_score(y_test, log_l1.predict(X_test))\n",
    "logl1_spec = specificity_score(y_test, log_l1.predict(X_test))\n",
    "logl1_acc = accuracy_score(y_test, log_l1.predict(X_test))\n",
    "logl1_prec = precision_score(y_test, log_l1.predict(X_test)) \n",
    "logl1_rec = recall_score(y_test, log_l1.predict(X_test))\n",
    "logl1_f1 = f1_score(y_test, log_l1.predict(X_test))\n",
    "logl1_sens, logl1_spec, logl1_acc, logl1_prec, logl1_rec, logl1_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce97c71",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c08e0fc",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3e5a0c",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0640e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix for tree\n",
    "tree_cm = confusion_matrix(y_true = y_test, y_pred = tree1.predict(X_test), labels = tree1.classes_)\n",
    "tree_disp = ConfusionMatrixDisplay(confusion_matrix= tree_cm, display_labels=tree1.classes_)\n",
    "tree_disp.plot()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d3e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c683ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96839114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sensitivity, specificity, and accuracy for decision tree\n",
    "tree1_sens = sensitivity_score(y_test, tree1.predict(X_test))\n",
    "tree1_spec = specificity_score(y_test, tree1.predict(X_test))\n",
    "tree1_acc = accuracy_score(y_test, tree1.predict(X_test))\n",
    "tree1_prec = precision_score(y_test, tree1.predict(X_test)) \n",
    "tree1_rec = recall_score(y_test, tree1.predict(X_test))\n",
    "tree1_f1 = f1_score(y_test, tree1.predict(X_test))\n",
    "tree1_sens, tree1_spec, tree1_acc, tree1_prec, tree1_rec, tree1_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d131cdc2",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01e7a63",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fb901c",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62de480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix for RF \n",
    "rf_cm = confusion_matrix(y_true = y_test, y_pred = random_forest.predict(X_test), labels = random_forest.classes_)\n",
    "# Create a ConfusionMatrixDisplay object for visualization\n",
    "rf_disp = ConfusionMatrixDisplay(confusion_matrix= rf_cm, display_labels=random_forest.classes_)\n",
    "rf_disp.plot()\n",
    "warnings.filterwarnings('ignore')\n",
    "#plt.show()\n",
    "#plt.savefig('rf_cm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e5d9db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d29f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588e2a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sensitivity, specificity, and accuracy for RF\n",
    "rf_sens = sensitivity_score(y_test, random_forest.predict(X_test))\n",
    "rf_spec = specificity_score(y_test, random_forest.predict(X_test))\n",
    "rf_acc = accuracy_score(y_test, random_forest.predict(X_test))\n",
    "rf_prec = precision_score(y_test, random_forest.predict(X_test)) \n",
    "rf_rec = recall_score(y_test, random_forest.predict(X_test))\n",
    "rf_f1 = f1_score(y_test, random_forest.predict(X_test))\n",
    "warnings.filterwarnings('ignore')\n",
    "rf_sens, rf_spec, rf_acc, rf_prec, rf_rec, rf_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fc5a82",
   "metadata": {},
   "source": [
    "## Gradient Boosted Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd27068b",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1a3977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af4ac914",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix for tree\n",
    "tree_cm = confusion_matrix(y_true = y_test, y_pred = tree_search.predict(X_test), labels = tree_search.classes_)\n",
    "tree_disp = ConfusionMatrixDisplay(confusion_matrix= tree_cm, display_labels=tree_search.classes_)\n",
    "tree_disp.plot()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "847ef522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cbc163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13d02e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sensitivity, specificity, and accuracy for decision tree \n",
    "tree_sens = sensitivity_score(y_test, tree_search.predict(X_test))\n",
    "tree_spec = specificity_score(y_test, tree_search.predict(X_test))\n",
    "tree_acc = accuracy_score(y_test, tree_search.predict(X_test))\n",
    "tree_prec = precision_score(y_test, tree_search.predict(X_test)) \n",
    "tree_rec = recall_score(y_test, tree_search.predict(X_test))\n",
    "tree_f1 = f1_score(y_test, tree_search.predict(X_test))\n",
    "tree_sens, tree_spec, tree_acc, tree_prec, tree_rec, tree_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307a6d35",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25a296f",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d3a17",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03ed381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "# Compute the confusion matrix for KNN \n",
    "knn_cm = confusion_matrix(y_true = y_test, y_pred = knn.predict(X_test), labels = knn.classes_)\n",
    "# Create a ConfusionMatrixDisplay object for visualization\n",
    "knn_disp = ConfusionMatrixDisplay(confusion_matrix= knn_cm, display_labels=knn.classes_)\n",
    "knn_disp.plot()\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3ff0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d11c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f428cfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sensitivity, specificity, and accuracy for KNN \n",
    "knn_sens = sensitivity_score(y_test, knn.predict(X_test))\n",
    "knn_spec = specificity_score(y_test, knn.predict(X_test))\n",
    "knn_acc = accuracy_score(y_test, knn.predict(X_test))\n",
    "knn_prec = precision_score(y_test, knn.predict(X_test)) \n",
    "knn_rec = recall_score(y_test, knn.predict(X_test))\n",
    "knn_f1 = f1_score(y_test, knn.predict(X_test))\n",
    "warnings.filterwarnings('ignore')\n",
    "knn_sens, knn_spec, knn_acc, knn_prec, knn_rec, knn_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb5d1fa",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428bd91b",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa662ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d2ef585",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12221328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750f1eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6de853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70539475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04f4d4fb",
   "metadata": {},
   "source": [
    "# Performance Metrics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedcdd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table to display performance metrics for different models on the validation dataset\n",
    "\n",
    "val_performance = [\n",
    "{'Model': 'Neural Net', 'Test Sensitivity': nn_sens, 'Test Specificity':nn_spec, \n",
    " 'Accuracy': nn_acc, 'Test Precision': nn_prec, 'Test Recall': nn_rec, 'F1 Score': nn_f1},\n",
    "    {'Model': 'Logistic Regression', 'Test Sensitivity': logl1_sens, 'Test Specificity':logl1_spec, \n",
    " 'Accuracy': logl1_acc, 'Test Precision': logl1_prec, 'Test Recall': logl1_rec,  'F1 Score': logl1_f1},\n",
    "    {'Model': 'Boosted Tree', 'Test Sensitivity': tree_sens, 'Test Specificity':tree_spec, \n",
    " 'Accuracy': tree_acc, 'Test Precision': tree_prec, 'Test Recall': tree_rec,  'F1 Score': tree_f1},\n",
    "    {'Model': 'Single Tree', 'Test Sensitivity': tree1_sens, 'Test Specificity':tree1_spec, \n",
    " 'Accuracy': tree1_acc, 'Test Precision': tree1_prec, 'Test Recall': tree1_rec, 'F1 Score': tree1_f1},\n",
    "    {'Model': 'Random Forest', 'Test Sensitivity': rf_sens, 'Test Specificity':rf_spec, \n",
    " 'Accuracy': rf_acc, 'Test Precision': rf_prec, 'Test Recall': rf_rec, 'F1 Score': rf_f1},\n",
    "    {'Model': 'K-Nearest Neighbors', 'Test Sensitivity': knn_sens, 'Test Specificity':knn_spec, \n",
    " 'Accuracy': knn_acc, 'Test Precision': knn_prec, 'Test Recall': knn_rec,  'F1 Score': knn_f1},\n",
    "    \n",
    "]\n",
    "# Create a formatted table using tabulate and specify the format as 'fancy_grid'\n",
    "table = tabulate(val_performance, headers='keys', tablefmt='fancy_grid')\n",
    "# Display the comparison table\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95523900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4541e117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b695f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
